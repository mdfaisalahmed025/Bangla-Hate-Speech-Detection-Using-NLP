{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BanglaHatedetection-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayoD9T7HFGCw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHFGJ9ym5vCJ",
        "outputId": "1f6855d6-6d99-4cc3-f7e1-ad3c998a6f04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.12.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.2.0)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2021.10.8)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator, constants\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "zFinfGk-5-CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()"
      ],
      "metadata": {
        "id": "lEX8z5xT6FQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# translate more than a phrase\n",
        "sentences = [\n",
        "    \"কুকুরটি খুশিতে বড় আপেল খেয়ে ফেলল\",\n",
        "    \"সবাই হলের মধ্যে চুপচাপ সেই ভালো বইটি পড়ে \",\n",
        "    \"বৃদ্ধ প্রধান শিক্ষক উচ্চস্বরে বলে দুষ্টু বাচ্চাদের কড়া ভাষায় তিরস্কার করলেন\",\n",
        "    \"আমি তোমাকে ভালোবাসি\"\n",
        "]\n",
        "translations = translator.translate(sentences, dest=\"en\")\n",
        "for translation in translations:\n",
        "    print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0YWX1yc6MY-",
        "outputId": "7dba6bdb-8dea-40a0-a164-c7fdea9368fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "কুকুরটি খুশিতে বড় আপেল খেয়ে ফেলল (bn) --> The dog happily ate the big apple (en)\n",
            "সবাই হলের মধ্যে চুপচাপ সেই ভালো বইটি পড়ে  (bn) --> Everyone in the hall read that good book quietly (en)\n",
            "বৃদ্ধ প্রধান শিক্ষক উচ্চস্বরে বলে দুষ্টু বাচ্চাদের কড়া ভাষায় তিরস্কার করলেন (bn) --> The old headmaster scolded the naughty children in a loud voice (en)\n",
            "আমি তোমাকে ভালোবাসি (bn) --> I love you (en)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5X52MPYk7-m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "other_words = [\"area\", \"book\", \"business\", \"case\", \"child\", \"company\", \"country\", \n",
        "               \"day\", \"eye\", \"fact\", \"family\", \"government\", \"group\", \"hand\", \"home\", \n",
        "               \"job\", \"life\", \"lot\", \"man\", \"money\", \"month\", \"mother\", \"food\", \"night\", \n",
        "               \"number\", \"part\", \"people\", \"place\", \"point\", \"problem\", \"program\", \n",
        "               \"question\", \"right\", \"room\", \"school\", \"state\", \"story\", \"student\", \n",
        "               \"study\", \"system\", \"thing\", \"time\", \"water\", \"way\", \"week\", \"woman\", \n",
        "               \"word\", \"work\", \"world\", \"year\", \"ask\", \"be\", \"become\", \"begin\", \"can\", \n",
        "               \"come\", \"do\", \"find\", \"get\", \"go\", \"have\", \"hear\", \"keep\", \"know\", \"let\", \n",
        "               \"like\", \"look\", \"make\", \"may\", \"mean\", \"might\", \"move\", \"play\", \"put\", \n",
        "               \"run\", \"say\", \"see\", \"seem\", \"should\", \"start\", \"think\", \"try\", \"turn\", \n",
        "               \"use\", \"want\", \"will\", \"work\", \"would\", \"asked\", \"was\", \"became\", \"began\", \n",
        "               \"can\", \"come\", \"do\", \"did\", \"found\", \"got\", \"went\", \"had\", \"heard\", \"kept\", \n",
        "               \"knew\", \"let\", \"liked\", \"looked\", \"made\", \"might\", \"meant\", \"might\", \"moved\", \n",
        "               \"played\", \"put\", \"ran\", \"said\", \"saw\", \"seemed\", \"should\", \"started\", \n",
        "               \"thought\", \"tried\", \"turned\", \"used\", \"wanted\" \"worked\", \"would\", \"able\", \n",
        "               \"bad\", \"best\", \"better\", \"big\", \"black\", \"certain\", \"clear\", \"different\", \n",
        "               \"early\", \"easy\", \"economic\", \"federal\", \"free\", \"full\", \"good\", \"great\", \n",
        "               \"hard\", \"high\", \"human\", \"important\", \"international\", \"large\", \"late\", \n",
        "               \"little\", \"local\", \"long\", \"low\", \"major\", \"military\", \"national\", \"new\", \n",
        "               \"old\", \"only\", \"other\", \"political\", \"possible\", \"public\", \"real\", \"recent\", \n",
        "               \"right\", \"small\", \"social\", \"special\", \"strong\", \"sure\", \"true\", \"white\", \n",
        "               \"whole\", \"young\", \"he\", \"she\", \"it\", \"they\", \"i\", \"my\", \"mine\", \"your\", \"his\", \n",
        "               \"her\", \"father\", \"mother\", \"dog\", \"cat\", \"cow\", \"tiger\", \"a\", \"about\", \"all\", \n",
        "               \"also\", \"and\", \"as\", \"at\", \"be\", \"because\", \"but\", \"by\", \"can\", \"come\", \"could\", \n",
        "               \"day\", \"do\", \"even\", \"find\", \"first\", \"for\", \"from\", \"get\", \"give\", \"go\", \n",
        "               \"have\", \"he\", \"her\", \"here\", \"him\", \"his\", \"how\", \"I\", \"if\", \"in\", \"into\", \n",
        "               \"it\", \"its\", \"just\", \"know\", \"like\", \"look\", \"make\", \"man\", \"many\", \"me\", \n",
        "               \"more\", \"my\", \"new\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \n",
        "               \"other\", \"our\", \"out\", \"people\", \"say\", \"see\", \"she\", \"so\", \"some\", \"take\", \n",
        "               \"tell\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \n",
        "               \"they\", \"thing\", \"think\", \"this\", \"those\", \"time\", \"to\", \"two\", \"up\", \"use\", \n",
        "               \"very\", \"want\", \"way\", \"we\", \"well\", \"what\", \"when\", \"which\", \"who\", \"will\", \n",
        "               \"with\", \"would\", \"year\", \"you\", \"your\"]\n",
        "translations = translator.translate(other_words, dest=\"bn\")\n",
        "for translation in translations:\n",
        "    print(f\"{translation.origin} ({translation.src}) --> {translation.text} ({translation.dest})\")              "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFbEmLfE7z2F",
        "outputId": "44b323e6-60b5-448e-eb86-0acdb6d48b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "area (en) --> এলাকা (bn)\n",
            "book (en) --> বই (bn)\n",
            "business (en) --> ব্যবসা (bn)\n",
            "case (en) --> মামলা (bn)\n",
            "child (en) --> শিশু (bn)\n",
            "company (en) --> প্রতিষ্ঠান (bn)\n",
            "country (en) --> দেশ (bn)\n",
            "day (en) --> দিন (bn)\n",
            "eye (en) --> চোখ (bn)\n",
            "fact (en) --> সত্য (bn)\n",
            "family (en) --> পরিবার (bn)\n",
            "government (en) --> সরকার (bn)\n",
            "group (en) --> দল (bn)\n",
            "hand (en) --> হাত (bn)\n",
            "home (en) --> বাড়ি (bn)\n",
            "job (en) --> চাকরি (bn)\n",
            "life (en) --> জীবন (bn)\n",
            "lot (en) --> অনেক (bn)\n",
            "man (en) --> মানুষ (bn)\n",
            "money (bn) --> মানি (bn)\n",
            "month (en) --> মাস (bn)\n",
            "mother (en) --> মা (bn)\n",
            "food (en) --> খাদ্য (bn)\n",
            "night (en) --> রাত (bn)\n",
            "number (en) --> সংখ্যা (bn)\n",
            "part (en) --> অংশ (bn)\n",
            "people (en) --> মানুষ (bn)\n",
            "place (en) --> স্থান (bn)\n",
            "point (en) --> বিন্দু (bn)\n",
            "problem (en) --> সমস্যা (bn)\n",
            "program (en) --> কার্যক্রম (bn)\n",
            "question (en) --> প্রশ্ন (bn)\n",
            "right (en) --> অধিকার (bn)\n",
            "room (en) --> রুম (bn)\n",
            "school (en) --> বিদ্যালয় (bn)\n",
            "state (en) --> অবস্থা (bn)\n",
            "story (en) --> গল্প (bn)\n",
            "student (en) --> ছাত্র (bn)\n",
            "study (en) --> অধ্যয়ন (bn)\n",
            "system (en) --> পদ্ধতি (bn)\n",
            "thing (en) --> জিনিস (bn)\n",
            "time (en) --> সময় (bn)\n",
            "water (en) --> জল (bn)\n",
            "way (en) --> উপায় (bn)\n",
            "week (en) --> সপ্তাহ (bn)\n",
            "woman (en) --> মহিলা (bn)\n",
            "word (en) --> শব্দ (bn)\n",
            "work (en) --> কাজ (bn)\n",
            "world (en) --> বিশ্ব (bn)\n",
            "year (en) --> বছর (bn)\n",
            "ask (en) --> জিজ্ঞাসা (bn)\n",
            "be (en) --> থাকা (bn)\n",
            "become (en) --> হয়ে (bn)\n",
            "begin (en) --> শুরু (bn)\n",
            "can (en) --> করতে পারা (bn)\n",
            "come (en) --> আসা (bn)\n",
            "do (en) --> করতে (bn)\n",
            "find (en) --> অনুসন্ধান (bn)\n",
            "get (en) --> পাওয়া (bn)\n",
            "go (en) --> যাওয়া (bn)\n",
            "have (en) --> আছে (bn)\n",
            "hear (en) --> শুনতে (bn)\n",
            "keep (en) --> রাখা (bn)\n",
            "know (en) --> জানি (bn)\n",
            "let (en) --> দিন (bn)\n",
            "like (en) --> পছন্দ (bn)\n",
            "look (en) --> তাকান (bn)\n",
            "make (en) --> করা (bn)\n",
            "may (en) --> হতে পারে (bn)\n",
            "mean (en) --> মানে (bn)\n",
            "might (en) --> হতে পারে (bn)\n",
            "move (en) --> সরানো (bn)\n",
            "play (en) --> খেলা (bn)\n",
            "put (en) --> রাখা (bn)\n",
            "run (en) --> চালান (bn)\n",
            "say (en) --> বল (bn)\n",
            "see (en) --> দেখা (bn)\n",
            "seem (en) --> মনে হয় (bn)\n",
            "should (en) --> উচিত (bn)\n",
            "start (en) --> শুরু (bn)\n",
            "think (en) --> মনে (bn)\n",
            "try (en) --> চেষ্টা করুন (bn)\n",
            "turn (en) --> পালা (bn)\n",
            "use (en) --> ব্যবহার (bn)\n",
            "want (en) --> চাই (bn)\n",
            "will (en) --> ইচ্ছাশক্তি (bn)\n",
            "work (en) --> কাজ (bn)\n",
            "would (en) --> হবে (bn)\n",
            "asked (en) --> জিজ্ঞাসা (bn)\n",
            "was (en) --> ছিল (bn)\n",
            "became (en) --> হয়ে ওঠে (bn)\n",
            "began (en) --> শুরু (bn)\n",
            "can (en) --> করতে পারা (bn)\n",
            "come (en) --> আসা (bn)\n",
            "do (en) --> করতে (bn)\n",
            "did (en) --> করেছিল (bn)\n",
            "found (en) --> পাওয়া গেছে (bn)\n",
            "got (en) --> পেয়েছি (bn)\n",
            "went (en) --> গিয়েছিলাম (bn)\n",
            "had (en) --> ছিল (bn)\n",
            "heard (en) --> শুনেছি (bn)\n",
            "kept (en) --> রাখা (bn)\n",
            "knew (en) --> জানতাম (bn)\n",
            "let (en) --> দিন (bn)\n",
            "liked (en) --> পছন্দ (bn)\n",
            "looked (en) --> লাগছিল (bn)\n",
            "made (en) --> তৈরি (bn)\n",
            "might (en) --> হতে পারে (bn)\n",
            "meant (en) --> মানে (bn)\n",
            "might (en) --> হতে পারে (bn)\n",
            "moved (en) --> সরানো (bn)\n",
            "played (en) --> খেলা (bn)\n",
            "put (en) --> রাখা (bn)\n",
            "ran (en) --> দৌড়ে (bn)\n",
            "said (en) --> বলেছেন (bn)\n",
            "saw (en) --> দেখেছি (bn)\n",
            "seemed (en) --> লাগছিল (bn)\n",
            "should (en) --> উচিত (bn)\n",
            "started (en) --> শুরু (bn)\n",
            "thought (en) --> চিন্তা (bn)\n",
            "tried (en) --> চেষ্টা (bn)\n",
            "turned (en) --> পরিণত (bn)\n",
            "used (en) --> ব্যবহৃত (bn)\n",
            "wantedworked (en) --> ওয়ান্টেড ওয়ার্কড (bn)\n",
            "would (en) --> হবে (bn)\n",
            "able (en) --> সক্ষম (bn)\n",
            "bad (en) --> খারাপ (bn)\n",
            "best (en) --> সেরা (bn)\n",
            "better (en) --> উত্তম (bn)\n",
            "big (en) --> বিশাল (bn)\n",
            "black (en) --> কালো (bn)\n",
            "certain (en) --> নিশ্চিত (bn)\n",
            "clear (en) --> পরিষ্কার (bn)\n",
            "different (en) --> ভিন্ন (bn)\n",
            "early (en) --> তাড়াতাড়ি (bn)\n",
            "easy (en) --> সহজ (bn)\n",
            "economic (en) --> অর্থনৈতিক (bn)\n",
            "federal (en) --> ফেডারেল (bn)\n",
            "free (en) --> বিনামূল্যে (bn)\n",
            "full (en) --> সম্পূর্ণ (bn)\n",
            "good (en) --> ভাল (bn)\n",
            "great (en) --> মহান (bn)\n",
            "hard (en) --> কঠিন (bn)\n",
            "high (en) --> উচ্চ (bn)\n",
            "human (en) --> মানব (bn)\n",
            "important (en) --> গুরুত্বপূর্ণ (bn)\n",
            "international (en) --> আন্তর্জাতিক (bn)\n",
            "large (en) --> বড় (bn)\n",
            "late (en) --> দেরী (bn)\n",
            "little (en) --> সামান্য (bn)\n",
            "local (en) --> স্থানীয় (bn)\n",
            "long (en) --> দীর্ঘ (bn)\n",
            "low (en) --> কম (bn)\n",
            "major (en) --> প্রধান (bn)\n",
            "military (en) --> সামরিক (bn)\n",
            "national (en) --> জাতীয় (bn)\n",
            "new (en) --> নতুন (bn)\n",
            "old (en) --> পুরাতন (bn)\n",
            "only (en) --> কেবল (bn)\n",
            "other (en) --> অন্যান্য (bn)\n",
            "political (en) --> রাজনৈতিক (bn)\n",
            "possible (en) --> সম্ভব (bn)\n",
            "public (en) --> পাবলিক (bn)\n",
            "real (en) --> বাস্তব (bn)\n",
            "recent (en) --> সাম্প্রতিক (bn)\n",
            "right (en) --> অধিকার (bn)\n",
            "small (en) --> ছোট (bn)\n",
            "social (en) --> সামাজিক (bn)\n",
            "special (en) --> বিশেষ (bn)\n",
            "strong (en) --> শক্তিশালী (bn)\n",
            "sure (en) --> নিশ্চিত (bn)\n",
            "true (en) --> সত্য (bn)\n",
            "white (en) --> সাদা (bn)\n",
            "whole (en) --> সম্পূর্ণ (bn)\n",
            "young (en) --> তরুণ (bn)\n",
            "he (en) --> তিনি (bn)\n",
            "she (en) --> সে (bn)\n",
            "it (en) --> এটা (bn)\n",
            "they (en) --> তারা (bn)\n",
            "i (en) --> i (bn)\n",
            "my (en) --> আমার (bn)\n",
            "mine (en) --> আমার (bn)\n",
            "your (en) --> তোমার (bn)\n",
            "his (en) --> তার (bn)\n",
            "her (en) --> তার (bn)\n",
            "father (en) --> পিতা (bn)\n",
            "mother (en) --> মা (bn)\n",
            "dog (en) --> কুকুর (bn)\n",
            "cat (en) --> বিড়াল (bn)\n",
            "cow (en) --> গাভী (bn)\n",
            "tiger (en) --> বাঘ (bn)\n",
            "a (en) --> ক (bn)\n",
            "about (en) --> সম্পর্কিত (bn)\n",
            "all (en) --> সব (bn)\n",
            "also (en) --> এছাড়াও (bn)\n",
            "and (en) --> এবং (bn)\n",
            "as (en) --> হিসাবে (bn)\n",
            "at (en) --> এ (bn)\n",
            "be (en) --> থাকা (bn)\n",
            "because (en) --> কারণ (bn)\n",
            "but (en) --> কিন্তু (bn)\n",
            "by (en) --> দ্বারা (bn)\n",
            "can (en) --> করতে পারা (bn)\n",
            "come (en) --> আসা (bn)\n",
            "could (en) --> পারে (bn)\n",
            "day (en) --> দিন (bn)\n",
            "do (en) --> করতে (bn)\n",
            "even (en) --> এমন কি (bn)\n",
            "find (en) --> অনুসন্ধান (bn)\n",
            "first (en) --> প্রথম (bn)\n",
            "for (en) --> জন্য (bn)\n",
            "from (en) --> থেকে (bn)\n",
            "get (en) --> পাওয়া (bn)\n",
            "give (en) --> দিতে (bn)\n",
            "go (en) --> যাওয়া (bn)\n",
            "have (en) --> আছে (bn)\n",
            "he (en) --> তিনি (bn)\n",
            "her (en) --> তার (bn)\n",
            "here (en) --> এখানে (bn)\n",
            "him (en) --> তাকে (bn)\n",
            "his (en) --> তার (bn)\n",
            "how (en) --> কিভাবে (bn)\n",
            "I (en) --> আমি (bn)\n",
            "if (en) --> যদি (bn)\n",
            "in (en) --> ভিতরে (bn)\n",
            "into (en) --> মধ্যে (bn)\n",
            "it (en) --> এটা (bn)\n",
            "its (en) --> এর (bn)\n",
            "just (en) --> শুধু (bn)\n",
            "know (en) --> জানি (bn)\n",
            "like (en) --> পছন্দ (bn)\n",
            "look (en) --> তাকান (bn)\n",
            "make (en) --> করা (bn)\n",
            "man (en) --> মানুষ (bn)\n",
            "many (en) --> অনেক (bn)\n",
            "me (en) --> আমাকে (bn)\n",
            "more (en) --> আরো (bn)\n",
            "my (en) --> আমার (bn)\n",
            "new (en) --> নতুন (bn)\n",
            "no (en) --> না (bn)\n",
            "not (en) --> না (bn)\n",
            "now (en) --> এখন (bn)\n",
            "of (en) --> এর (bn)\n",
            "on (en) --> চালু (bn)\n",
            "one (en) --> এক (bn)\n",
            "only (en) --> কেবল (bn)\n",
            "or (en) --> বা (bn)\n",
            "other (en) --> অন্যান্য (bn)\n",
            "our (en) --> আমাদের (bn)\n",
            "out (en) --> আউট (bn)\n",
            "people (en) --> মানুষ (bn)\n",
            "say (en) --> বল (bn)\n",
            "see (en) --> দেখা (bn)\n",
            "she (en) --> সে (bn)\n",
            "so (en) --> তাই (bn)\n",
            "some (en) --> কিছু (bn)\n",
            "take (en) --> গ্রহণ করা (bn)\n",
            "tell (en) --> বলুন (bn)\n",
            "than (en) --> চেয়ে (bn)\n",
            "that (en) --> যে (bn)\n",
            "the (en) --> দ্য (bn)\n",
            "their (en) --> তাদের (bn)\n",
            "them (en) --> তাদের (bn)\n",
            "then (en) --> তারপর (bn)\n",
            "there (en) --> সেখানে (bn)\n",
            "these (en) --> এইগুলো (bn)\n",
            "they (en) --> তারা (bn)\n",
            "thing (en) --> জিনিস (bn)\n",
            "think (en) --> মনে (bn)\n",
            "this (en) --> এই (bn)\n",
            "those (en) --> সেগুলো (bn)\n",
            "time (en) --> সময় (bn)\n",
            "to (en) --> প্রতি (bn)\n",
            "two (en) --> দুই (bn)\n",
            "up (en) --> আপ (bn)\n",
            "use (en) --> ব্যবহার (bn)\n",
            "very (en) --> খুব (bn)\n",
            "want (en) --> চাই (bn)\n",
            "way (en) --> উপায় (bn)\n",
            "we (en) --> আমরা (bn)\n",
            "well (en) --> আমরা হব (bn)\n",
            "what (en) --> কি (bn)\n",
            "when (en) --> কখন (bn)\n",
            "which (en) --> যা (bn)\n",
            "who (en) --> WHO (bn)\n",
            "will (en) --> ইচ্ছাশক্তি (bn)\n",
            "with (en) --> সঙ্গে (bn)\n",
            "would (en) --> হবে (bn)\n",
            "year (en) --> বছর (bn)\n",
            "you (en) --> আপনি (bn)\n",
            "your (en) --> তোমার (bn)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
        "    Output: a tensor containing the indexes of the word\"\"\"\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# This is the example training data\n",
        "training_data = [\n",
        "    (\"the dog happily ate the big apple\".split(), [\"DET\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\", \"NN\"]),\n",
        "    (\"everybody read that good book quietly in the hall\".split(), [\"NN\", \"V\", \"DET\", \"ADJ\", \"NN\", \"ADV\", \"PRP\", \"DET\", \"NN\"]),\n",
        "    (\"the old head master sternly scolded the naughty children for \\\n",
        "     being very loud\".split(), [\"DET\", \"ADJ\", \"ADJ\", \"NN\", \"ADV\", \"V\", \"DET\", \"ADJ\",  \"NN\", \"PRP\", \"V\", \"ADJ\", \"NN\"]),\n",
        "    (\"i love you loads\".split(), [\"PRN\", \"V\", \"PRN\", \"ADV\"])\n",
        "]\n",
        "#  These are other words which we would like to predict (within sentences) using the model\n",
        "other_words = [\"area\", \"book\", \"business\", \"case\", \"child\", \"company\", \"country\", \n",
        "               \"day\", \"eye\", \"fact\", \"family\", \"government\", \"group\", \"hand\", \"home\", \n",
        "               \"job\", \"life\", \"lot\", \"man\", \"money\", \"month\", \"mother\", \"food\", \"night\", \n",
        "               \"number\", \"part\", \"people\", \"place\", \"point\", \"problem\", \"program\", \n",
        "               \"question\", \"right\", \"room\", \"school\", \"state\", \"story\", \"student\", \n",
        "               \"study\", \"system\", \"thing\", \"time\", \"water\", \"way\", \"week\", \"woman\", \n",
        "               \"word\", \"work\", \"world\", \"year\", \"ask\", \"be\", \"become\", \"begin\", \"can\", \n",
        "               \"come\", \"do\", \"find\", \"get\", \"go\", \"have\", \"hear\", \"keep\", \"know\", \"let\", \n",
        "               \"like\", \"look\", \"make\", \"may\", \"mean\", \"might\", \"move\", \"play\", \"put\", \n",
        "               \"run\", \"say\", \"see\", \"seem\", \"should\", \"start\", \"think\", \"try\", \"turn\", \n",
        "               \"use\", \"want\", \"will\", \"work\", \"would\", \"asked\", \"was\", \"became\", \"began\", \n",
        "               \"can\", \"come\", \"do\", \"did\", \"found\", \"got\", \"went\", \"had\", \"heard\", \"kept\", \n",
        "               \"knew\", \"let\", \"liked\", \"looked\", \"made\", \"might\", \"meant\", \"might\", \"moved\", \n",
        "               \"played\", \"put\", \"ran\", \"said\", \"saw\", \"seemed\", \"should\", \"started\", \n",
        "               \"thought\", \"tried\", \"turned\", \"used\", \"wanted\" \"worked\", \"would\", \"able\", \n",
        "               \"bad\", \"best\", \"better\", \"big\", \"black\", \"certain\", \"clear\", \"different\", \n",
        "               \"early\", \"easy\", \"economic\", \"federal\", \"free\", \"full\", \"good\", \"great\", \n",
        "               \"hard\", \"high\", \"human\", \"important\", \"international\", \"large\", \"late\", \n",
        "               \"little\", \"local\", \"long\", \"low\", \"major\", \"military\", \"national\", \"new\", \n",
        "               \"old\", \"only\", \"other\", \"political\", \"possible\", \"public\", \"real\", \"recent\", \n",
        "               \"right\", \"small\", \"social\", \"special\", \"strong\", \"sure\", \"true\", \"white\", \n",
        "               \"whole\", \"young\", \"he\", \"she\", \"it\", \"they\", \"i\", \"my\", \"mine\", \"your\", \"his\", \n",
        "               \"her\", \"father\", \"mother\", \"dog\", \"cat\", \"cow\", \"tiger\", \"a\", \"about\", \"all\", \n",
        "               \"also\", \"and\", \"as\", \"at\", \"be\", \"because\", \"but\", \"by\", \"can\", \"come\", \"could\", \n",
        "               \"day\", \"do\", \"even\", \"find\", \"first\", \"for\", \"from\", \"get\", \"give\", \"go\", \n",
        "               \"have\", \"he\", \"her\", \"here\", \"him\", \"his\", \"how\", \"I\", \"if\", \"in\", \"into\", \n",
        "               \"it\", \"its\", \"just\", \"know\", \"like\", \"look\", \"make\", \"man\", \"many\", \"me\", \n",
        "               \"more\", \"my\", \"new\", \"no\", \"not\", \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \n",
        "               \"other\", \"our\", \"out\", \"people\", \"say\", \"see\", \"she\", \"so\", \"some\", \"take\", \n",
        "               \"tell\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \n",
        "               \"they\", \"thing\", \"think\", \"this\", \"those\", \"time\", \"to\", \"two\", \"up\", \"use\", \n",
        "               \"very\", \"want\", \"way\", \"we\", \"well\", \"what\", \"when\", \"which\", \"who\", \"will\", \n",
        "               \"with\", \"would\", \"year\", \"you\", \"your\"]\n",
        "\n",
        "word_to_ix = {} # This is the word dictionary which will contain the index to each word\n",
        "\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix.keys():\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "for word in other_words:\n",
        "    if word not in word_to_ix.keys():\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# print(word_to_ix) # Just have a look at what it contains\n",
        "\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2, \"ADJ\": 3, \"ADV\": 4, \"PRP\": 5, \"PRN\": 6} # This dictionary contains the indices of the tags\n",
        "\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_DIM = 64\n"
      ],
      "metadata": {
        "id": "swGzxs6FFUzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "CHC783RBFx-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Here I initialize the model with all the necesarry parameters\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix.keys()), len(tag_to_ix.keys()))\n",
        "\n",
        "# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "# We will be using a simple SGD optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# The test sentence\n",
        "seq1 = \"everybody read the book and ate the food\".split()\n",
        "seq2 = \"she want food\".split()\n",
        "print(\"Running a check on the model before training.\\nSentences:\\n{}\\n{}\".format(\" \".join(seq1), \" \".join(seq2)))\n",
        "with torch.no_grad():\n",
        "    for seq in [seq1, seq2]:\n",
        "        inputs = prepare_sequence(seq, word_to_ix)\n",
        "        tag_scores = model(inputs)\n",
        "        _, indices = torch.max(tag_scores, 1)\n",
        "        ret = []\n",
        "        for i in range(len(indices)):\n",
        "            for key, value in tag_to_ix.items():\n",
        "                if indices[i] == value:\n",
        "                    ret.append((seq[i], key))\n",
        "        print(ret)\n",
        "    \n",
        "print(\"Training Started\")\n",
        "for epoch in range(300):\n",
        "    for sentence, tags in training_data:\n",
        "        model.zero_grad()\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "        \n",
        "        tag_scores = model(sentence_in)\n",
        "        \n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print(\"Training Finished!!!\\nAgain testing on unknown data\")\n",
        "with torch.no_grad():\n",
        "    for seq in [seq1, seq2]:\n",
        "        inputs = prepare_sequence(seq, word_to_ix)\n",
        "        tag_scores = model(inputs)\n",
        "        _, indices = torch.max(tag_scores, 1)\n",
        "        ret = []\n",
        "        for i in range(len(indices)):\n",
        "            for key, value in tag_to_ix.items():\n",
        "                if indices[i] == value:\n",
        "                    ret.append((seq[i], key))\n",
        "        print(ret)\n",
        "        \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVXX_DoCF_Kb",
        "outputId": "ab0d1d30-ed09-4f1f-d68c-128469abcaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running a check on the model before training.\n",
            "Sentences:\n",
            "everybody read the book and ate the food\n",
            "she want food\n",
            "[('everybody', 'ADJ'), ('read', 'ADV'), ('the', 'ADJ'), ('book', 'ADJ'), ('and', 'V'), ('ate', 'V'), ('the', 'ADJ'), ('food', 'ADJ')]\n",
            "[('she', 'ADJ'), ('want', 'ADV'), ('food', 'ADJ')]\n",
            "Training Started\n",
            "Training Finished!!!\n",
            "Again testing on unknown data\n",
            "[('everybody', 'NN'), ('read', 'V'), ('the', 'DET'), ('book', 'NN'), ('and', 'NN'), ('ate', 'V'), ('the', 'DET'), ('food', 'ADJ')]\n",
            "[('she', 'DET'), ('want', 'V'), ('food', 'V')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jXyuflqJ5KZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    \"\"\"Input: takes in a list of words, and a dictionary containing the index of the words\n",
        "    Output: a tensor containing the indexes of the word\"\"\"\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# This is the example training data\n",
        "training_data = [\n",
        "    (\"কুকুরটি খুশিতে বড় আপেল খেয়ে ফেলল\".split(), [ \"বিশেষ্য\", \"ক্রিয়া বিশেষণ\", \"ক্রিয়া\", \"নির্ধারক\", \"নাম বিশেষণ\", \"বিশেষ্য\"]),\n",
        "       (\"সে খাবার চায়\".split(), [ \"বিশেষ্য\",  \"নাম বিশেষণ\", \"বিশেষ্য\"]),\n",
        "     \n",
        "]\n",
        "#  These are other words which we would like to predict (within sentences) using the model\n",
        "other_words = [\"কুকুরটি\" , \"খুশিতে\", \" বড়\", \" আপেল\", \"খেয়ে\", \" ফেলল\",\"সে\" , \"খাবার\",  \"চায়\"]\n",
        "\n",
        "word_to_ix = {} # This is the word dictionary which will contain the index to each word\n",
        "\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix.keys():\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "for word in other_words:\n",
        "    if word not in word_to_ix.keys():\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "# print(word_to_ix) # Just have a look at what it contains\n",
        "\n",
        "tag_to_ix = {\"নির্ধারক\": 0, \"বিশেষ্য\": 1, \"ক্রিয়া\": 2, \"নাম বিশেষণ\": 3, \"ক্রিয়া বিশেষণ\": 4, \"অব্যয়\": 5, \"সর্বনাম\": 6} # This dictionary contains the indices of the tags\n",
        "\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_DIM = 64"
      ],
      "metadata": {
        "id": "ch53HSmvL3FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        \n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ],
      "metadata": {
        "id": "ZXGhd8IGOZMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Here I initialize the model with all the necesarry parameters\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix.keys()), len(tag_to_ix.keys()))\n",
        "\n",
        "# Define the loss function as the Negative Log Likelihood loss (NLLLoss)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "# We will be using a simple SGD optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# The test sentence\n",
        "seq1 = \"কুকুরটি খুশিতে বড় আপেল খেয়ে ফেলল\".split()\n",
        "seq2 = \"সে খাবার চায়\".split()\n",
        "print(\"Running a check on the model before training.\\nSentences:\\n{}\\n{}\".format(\" \".join(seq1), \" \".join(seq2)))\n",
        "with torch.no_grad():\n",
        "    for seq in [seq1, seq2]:\n",
        "        inputs = prepare_sequence(seq, word_to_ix)\n",
        "        tag_scores = model(inputs)\n",
        "        _, indices = torch.max(tag_scores, 1)\n",
        "        ret = []\n",
        "        for i in range(len(indices)):\n",
        "            for key, value in tag_to_ix.items():\n",
        "                if indices[i] == value:\n",
        "                    ret.append((seq[i], key))\n",
        "        print(ret)\n",
        "    \n",
        "print(\"Training Started\")\n",
        "for epoch in range(300):\n",
        "    for sentence, tags in training_data:\n",
        "        model.zero_grad()\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "        \n",
        "        tag_scores = model(sentence_in)\n",
        "        \n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "print(\"Training Finished!!!\\nAgain testing on unknown data\")\n",
        "with torch.no_grad():\n",
        "    for seq in [seq1, seq2]:\n",
        "        inputs = prepare_sequence(seq, word_to_ix)\n",
        "        tag_scores = model(inputs)\n",
        "        _, indices = torch.max(tag_scores, 1)\n",
        "        ret = []\n",
        "        for i in range(len(indices)):\n",
        "            for key, value in tag_to_ix.items():\n",
        "                if indices[i] == value:\n",
        "                    ret.append((seq[i], key))\n",
        "        print(ret)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOjld4CpO6Cg",
        "outputId": "5d98cb41-68c4-4e1d-cbd7-79e8b7bd4ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running a check on the model before training.\n",
            "Sentences:\n",
            "কুকুরটি খুশিতে বড় আপেল খেয়ে ফেলল\n",
            "সে খাবার চায়\n",
            "[('কুকুরটি', 'নাম বিশেষণ'), ('খুশিতে', 'নাম বিশেষণ'), ('বড়', 'সর্বনাম'), ('আপেল', 'সর্বনাম'), ('খেয়ে', 'সর্বনাম'), ('ফেলল', 'সর্বনাম')]\n",
            "[('সে', 'সর্বনাম'), ('খাবার', 'সর্বনাম'), ('চায়', 'সর্বনাম')]\n",
            "Training Started\n",
            "Training Finished!!!\n",
            "Again testing on unknown data\n",
            "[('কুকুরটি', 'বিশেষ্য'), ('খুশিতে', 'ক্রিয়া বিশেষণ'), ('বড়', 'ক্রিয়া'), ('আপেল', 'নির্ধারক'), ('খেয়ে', 'নাম বিশেষণ'), ('ফেলল', 'বিশেষ্য')]\n",
            "[('সে', 'বিশেষ্য'), ('খাবার', 'নাম বিশেষণ'), ('চায়', 'বিশেষ্য')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BNLP \n"
      ],
      "metadata": {
        "id": "Hmbw-PnBS739"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHALoioATfbt",
        "outputId": "25dde463-ccea-4145-e963-9c587fbdec82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox_Bn0-LvD7v",
        "outputId": "0bb2fa55-d188-49b2-ecbd-4f75e64fb182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 4.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.11.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 478 kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.11.0\n",
            "    Uninstalling tokenizers-0.11.0:\n",
            "      Successfully uninstalled tokenizers-0.11.0\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer(\"/content/englishtranslate.txt\")\n",
        "tokenized_sequence = tokenizer.encode(sequence)\n",
        "print(tokenized_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "k2xLtO_hviGC",
        "outputId": "becf6d34-f7f1-4654-9c46-c4232338c99a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d60e17b511c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/englishtranslate.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenized_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sequence' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "sequence = \"Hello, y'all! How are you Tokenizer 😁 ?\"\n",
        "tokenizerBW = BertWordPieceTokenizer(\"/content/englishtranslate.txt\")\n",
        "tokenized_sequenceBW = tokenizerBW.encode(sequence)\n",
        "print(tokenized_sequenceBW)\n",
        "print(type(tokenized_sequenceBW))\n",
        "print(tokenized_sequenceBW.ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "SKvbvbP2wiJO",
        "outputId": "9a05559c-063d-4920-9235-629a41507efb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-51a2bc3f72a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, y'all! How are you Tokenizer 😁 ?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizerBW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertWordPieceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/englishtranslate.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenized_sequenceBW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizerBW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sequenceBW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tokenizers/implementations/bert_wordpiece.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, unk_token, sep_token, cls_token, pad_token, mask_token, clean_text, handle_chinese_chars, strip_accents, lowercase, wordpieces_prefix)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0msep_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msep_token_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sep_token not found in the vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mcls_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls_token_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sep_token not found in the vocabulary"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import BasicTokenizer\n",
        "basic_tokenizer = BasicTokenizer()\n",
        "raw_text = \"কুকুরটি খুশিতে বড় আপেল খেয়ে ফেলল।\"\n",
        "tokens = basic_tokenizer.tokenize(raw_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiGYvohwTk40",
        "outputId": "7354c4f2-c0ea-454c-b4da-b9f65cc18cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['কুকুরটি', 'খুশিতে', 'বড়', 'আপেল', 'খেয়ে', 'ফেলল', '।']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import NLTKTokenizer\n",
        "\n",
        "bnltk = NLTKTokenizer()\n",
        "text = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ?\"\n",
        "word_tokens = bnltk.word_tokenize(text)\n",
        "sentence_tokens = bnltk.sentence_tokenize(text)\n",
        "print(word_tokens)\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b83mQFkCT3Jl",
        "outputId": "8d825dc9-406b-452f-8fbd-9ee44a0c29cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['আমি', 'ভাত', 'খাই', '।', 'সে', 'বাজারে', 'যায়', '।', 'তিনি', 'কি', 'সত্যিই', 'ভালো', 'মানুষ', '?']\n",
            "['আমি ভাত খাই।', 'সে বাজারে যায়।', 'তিনি কি সত্যিই ভালো মানুষ?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import SentencepieceTokenizer\n",
        "\n",
        "bsp = SentencepieceTokenizer()\n",
        "model_path = \"./model/bn_spm.model\"\n",
        "input_text = \"আমি ভাত খাই। সে বাজারে যায়।\"\n",
        "tokens = bsp.tokenize(model_path, input_text)\n",
        "print(tokens)\n",
        "text2id = bsp.text2id(model_path, input_text)\n",
        "print(text2id)\n",
        "id2text = bsp.id2text(model_path, text2id)\n",
        "print(id2text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "dIPUBrktT887",
        "outputId": "80987106-0c57-4df4-e6d2-9215a8595e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ddea709f2b96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./model/bn_spm.model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"আমি ভাত খাই। সে বাজারে যায়।\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtext2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext2id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bnlp/tokenizer/sentencepiece.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, model_path, text)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[1;32m     24\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbsp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, model_file, model_proto)\u001b[0m\n\u001b[1;32m    365\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmodel_proto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromSerializedProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_proto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mLoadFromFile\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_LoadFromFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDecodeIdsWithCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Not found: \"./model/bn_spm.model\": No such file or directory Error #2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import BengaliWord2Vec\n",
        "\n",
        "bwv = BengaliWord2Vec()\n",
        "model_path = \"bengali_word2vec.model\"\n",
        "word = 'গ্রাম'\n",
        "vector = bwv.generate_word_vector(model_path, word)\n",
        "print(vector.shape)\n",
        "print(vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "XakWTPTCUWw0",
        "outputId": "81e4be40-a323-4be2-f014-ddaf1c34c4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ec0c949dd4f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bengali_word2vec.model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'গ্রাম'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_word_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bnlp/embedding/word2vec.py\u001b[0m in \u001b[0;36mgenerate_word_vector\u001b[0;34m(self, model_path, input_word)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \"\"\"\n\u001b[1;32m   1921\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1922\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1923\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m                 \u001b[0mrethrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_lifecycle_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \"\"\"\n\u001b[0;32m-> 1457\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1458\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# needed because loading from S3 doesn't support readline()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bengali_word2vec.model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bnlp import POS\n",
        "bn_pos = POS()\n",
        "model_path = \"model/bn_pos.pkl\"\n",
        "text = \"আমি ভাত খাই।\" # or you can pass ['আমি', 'ভাত', 'খাই', '।']\n",
        "res = bn_pos.tag(model_path, text)\n",
        "print(res)\n",
        "# [('আমি', 'PPR'), ('ভাত', 'NC'), ('খাই', 'VM'), ('।', 'PU')]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "EVU05z8RVB0q",
        "outputId": "1704ffdd-0922-4806-a77d-c5344caa9eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1b3a6ad39cc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model/bn_pos.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"আমি ভাত খাই।\"\u001b[0m \u001b[0;31m# or you can pass ['আমি', 'ভাত', 'খাই', '।']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# [('আমি', 'PPR'), ('ভাত', 'NC'), ('খাই', 'VM'), ('।', 'PU')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bnlp/pos.py\u001b[0m in \u001b[0;36mtag\u001b[0;34m(self, model_path, text)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPOS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/bn_pos.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_ix = {\"she\": 0, \"want\": 1,\"food\":2}\n",
        "embeds = nn.Embedding(3, 5)  # 2 words in vocab, 5 dimensional embeddings\n",
        "lookup_tensor = torch.tensor([word_to_ix[\"she\"]], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1-_fs2QIXZl",
        "outputId": "12cd2d67-a7b5-45ad-c629-02daf57a1898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0825, -0.1308, -0.7487, -0.5894, -0.3614]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "training_data = [\n",
        "    # Tags are: DET - determiner; NN - noun; V - verb\n",
        "    # For example, the word \"The\" is a determiner\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
        "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
        "\n",
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "print(tag_to_ix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SfOS4V-VHYC",
        "outputId": "07827b9f-e7f1-4f96-9820-eb6a98bd82aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
            "{'DET': 0, 'NN': 1, 'V': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2\n",
        "EMBEDDING_DIM = 10\n",
        "# We will use Shakespeare Sonnet 2\n",
        "test_sentence = \"\"\"বরিশাল সেপ্টেম্বর ১২ বিডিনিউজ টোয়েন্টিফোর ডটকম উচ্চশব্দে গান বাজানোর প্রতিবাদ করায় নরসিংদীতে একজন খুনের অভিযোগের পরদিন বরিশালে এ ভাবে গান বাজানো নিষিদ্ধ করেছে পুলিশ\n",
        "বরিশাল পুলিশ কমিশনার আব্দুর রহিমের নির্দেশে নগরীতে উচ্চশব্দে গান বাজানো নিষিদ্ধ করা হয়েছে বলে রোববার বিডিনিউজ টোয়েন্টিফোর ডটকমকে জানান কোতয়ালী মডেল থানার ওসি কমলেশ চন্দ্র হালদার\n",
        "এদিন উচ্চশব্দে গান বাজানোয় নগরীর দুই স্থান থেকে উচ্চশব্দযন্ত্র আটক করা হয়েছে বলেও জানান ওসি\n",
        "ঈদ উপলক্ষে উচ্চস্বরে গান বাজানোয় বাধা দেয়ায় নরসিংদীতে লোকমান হোসেন নামে এক ব্যক্তিকে এক ইউপি সদস্যের আত্মীয় কুপিয়ে খুন করেন শনিবার বলে অভিযোগ করেন নিহতের বাবা\"\"\".split()\n",
        "# we should tokenize the input, but we will ignore that for now\n",
        "# build a list of tuples.\n",
        "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
        "ngrams = [\n",
        "    (\n",
        "        [test_sentence[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
        "        test_sentence[i]\n",
        "    )\n",
        "    for i in range(CONTEXT_SIZE, len(test_sentence))\n",
        "]\n",
        "# Print the first 3, just so you can see what they look like.\n",
        "print(ngrams[:3])\n",
        "\n",
        "vocab = set(test_sentence)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "class NGramLanguageModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(NGramLanguageModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))\n",
        "        out = F.relu(self.linear1(embeds))\n",
        "        out = self.linear2(out)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        return log_probs\n",
        "\n",
        "\n",
        "losses = []\n",
        "loss_function = nn.NLLLoss()\n",
        "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for context, target in ngrams:\n",
        "\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "    losses.append(total_loss)\n",
        "print(losses)  # The loss decreased every iteration over the training data!\n",
        "\n",
        "# To get the embedding of a particular word, e.g. \"beauty\"\n",
        "print(model.embeddings.weight[word_to_ix[\"বরিশাল\"]])"
      ],
      "metadata": {
        "id": "fCWEduJrQG3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b03d90-ea9a-4ca9-c9d5-51310292db6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['সেপ্টেম্বর', 'বরিশাল'], '১২'), (['১২', 'সেপ্টেম্বর'], 'বিডিনিউজ'), (['বিডিনিউজ', '১২'], 'টোয়েন্টিফোর')]\n",
            "[379.95861530303955, 378.02120447158813, 376.0956861972809, 374.18201875686646, 372.28060388565063, 370.39004826545715, 368.51042580604553, 366.6396281719208, 364.77654218673706, 362.921204328537]\n",
            "tensor([-0.3219,  1.0425,  1.0377, -0.1977, -0.3490, -1.3245, -1.0885, -0.5157,\n",
            "         1.1576,  1.3207], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
        "\n",
        "# By deriving a set from `raw_text`, we deduplicate the array\n",
        "vocab = set(raw_text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "data = []\n",
        "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
        "    context = (\n",
        "        [raw_text[i - j - 1] for j in range(CONTEXT_SIZE)]\n",
        "        + [raw_text[i + j + 1] for j in range(CONTEXT_SIZE)]\n",
        "    )\n",
        "    target = raw_text[i]\n",
        "    data.append((context, target))\n",
        "print(data[:5])\n",
        "\n",
        "\n",
        "class CBOW(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "# Create your model and train. Here are some functions to help you make\n",
        "# the data ready for use by your module.\n",
        "\n",
        "\n",
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "make_context_vector(data[0][0], word_to_ix) "
      ],
      "metadata": {
        "id": "AOTVEJS-craA",
        "outputId": "b4d25526-fb00-448a-ada5-aed9a8cae988",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['are', 'We', 'to', 'study'], 'about'), (['about', 'are', 'study', 'the'], 'to'), (['to', 'about', 'the', 'idea'], 'study'), (['study', 'to', 'idea', 'of'], 'the'), (['the', 'study', 'of', 'a'], 'idea')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([48, 27, 10, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}